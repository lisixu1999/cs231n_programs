# Lecture 3: Loss functions and Optimization
## 1. Loss Function
### 1.1 回顾
&nbsp; &nbsp; 我们再上一次说到，我们希望找到一个权重$W$使得我们的识别效果最好，怎么算好呢，我们需要一个评估标准。这就是Loss Function（译名：损失函数，代价函数，目标函数）
### 1.2 多类支持向量机损失 Multiclass Support Vector Machine Loss
&nbsp; &nbsp; 我们首先给出这个函数的形式：假设我们有很多图片来做训练，那么，其中第$i$张图片包含有像素数据$x_i$和代表正确分类的标签$y_i$。我们令$s = f(x_i,W)$那么$s_j$就是预测出的第j项的得分。对这一个训练数据的Cost Function定义为。
$$ L_i = \sum_{j\not= y_i}max(0,s_j - s_{y_i} + \Delta) $$
举一个比较具体的例子。假设我们经过计算，得出三个类别的评分分别为$【3.2,5.1,-1.7】$那么，并且第一个标签为正确分类，且超参数$\Delta = 1$那么。
$$L = max(0,5.1-3.2 + 1) + max(0,-1.7-3.2 + 1) = 2.9$$
另外这里需要注意$\Delta$其实是规定的一个边界值，这个比如说就是说正确分类和错误分类评分差值达到多少时，本函数开始敏感，其结果就是其实设置成为多少不是很重要，这里的差值会使得权重向一个整倍数方向移动，通常$\Delta = 1$就可以了。

如果训练集有很多数据，那么整体结果是：
$$L = \frac{1}{N}\sum_{i = 1}^N L_i$$
（此处有另一种标记方式，在吴恩达的那个五部分的课程里有用到，将单个训练样本的损失称作Loss Function，将一次训练的多个数据的损失称为Cost Function，但表达的意思是一样的，……起码我理解的是一样的，本文均使用Loss Function）
几个小问题
* Q1：我们在上面的式子里有$j\not=y$如果等了会怎么样？
* Q2：我们不求和但是使用平均值会怎么样？$L_i = \frac{1}{N-1}\sum_{j\not= y_i}max(0,s_j - s_{y_i} + \Delta)$
* Q3：我们使用$ L_i = \sum_{j\not= y_i}max(0,s_j - s_{y_i} + \Delta)^2 $怎么样
* Q4：这个损失函数的最大值和最小值是什么？
* Q5：很多时候权重会被初始化为比较小的值，那一开始损失函数会是怎么样的？

### 1.3  正则化 Regularization
&nbsp; &nbsp; 有些权重是我们不喜欢的，比如我们令$x = [1,1,1,1]$,$W_1 = [1,0,0,0]$,$W_2 = [0.25,0.25,0.25,0.25]$那么我们会得到$W_1^Tx = W_2^Tx = 1$但是，显然前者只利用了数据的很少一部分，这就会使得在面对未知数据时前者更可能会出错，所以，在得到同样准确度时，我们更希望是后者。因此，我们在Loss Funnction中添加一项正则化损失，以体现我们的意愿。
* L2 regularization：$R(W) = \sum_j\sum_kW_{j,k}^2$
* L1 regularization：$R(W) = \sum_j\sum_k|W_{j,k}|$
因此，整理后的Loss Function 为
$$ L = \frac{1}{N}\sum_{i = 1}^N L_i + \lambda R(W)$$
其中$\lambda$为超参数

### 1.4 Softmax 分类器(Multinomial Logistic Regression)
&nbsp; &nbsp; 这个应该是跟概率论有比较大的关系的，但是我现在还没学到（顺便苦逼一句，概率论选到了英文授课）。所以我们先把式子摆出来然后再实际计算看下吧。
$$L_i = -log(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}})$$
跟上面一样$j$为预测结果的索引，另外，其中$\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}$称作Softmax函数。
下面是一个具体的计算例子，假设我们有一个$s = [3.2,5.1,-1.7]$那么我们首先取指数，这一步是使得所有的均为正数，那么$e^s = [24.5,164.0,0.18]$然后我们在把它们都转换到$[0,1]$当中其实是开区间，但是考虑到精度，就闭了。怎么转换，就除以和就行了，本例中$24.5+164.0+0.18=188.68$因此转换之后的数值为$[0.13,0.84,0]$，最后$L_i = -log(0.13) = 0.89$
&nbsp; &nbsp; 这里的中间步骤$[0.13,0.84,0]$可以理解为分类概率，即模型认为这张图片是类别1,2,3的概率分别是13%，84%和0%。
小问题：
* Q6：这个损失函数最大值最小值是什么？
* Q7：很多时候权重会被初始化为比较小的值，那一开始损失函数会是怎么样的？

### 1.5 Softmax和多类SVM的对比
&nbsp; &nbsp; 我们使用$[10,-100,-100]$作为例子，其中假设类别一是正确分类
$$L_{SVM} = max(0,-109) + max(0,-109) = 0$$
$$L_{Softmax} = -log(1) = 0$$
我们注意到，当我们改变另外两个类别的数值的时候比如改变成$[10,8,8]$，多类SVM损失函数的数值其实没有变化，但是Softmax变化了
$$L_{SVM} = max(0,-1) + max(0,-1) = 0$$
$$L_{Softmax} = -log(0.7869) = 0.104$$
为什么，就是当初那个$\Delta$控制着边界条件，多类SVM只对边界值敏感，但是对差距比较大的数值不敏感，但是显然，Softmax仍然会受到影响，虽然比较少，但是我们有一种感性的认知，Softmax分类器是考虑了全部结果的，只有预测结果为$[1,0,0]$这种结果是Softmax才会为0，但是多类SVM只要目标标签分数比其他的大1，就会为0.

### 1.6 上面那几个小问题的答案
* Q1：会在最后结果上整体加一。
* Q2：其实没什么，只是除了一个常数值。
* Q3：这个函数是原函数的非线性变形，相当于两个损失函数，至于选择哪个，这是个超参。
* Q4：范围是$[0,+\infty)$
* Q5：结果会是3（分类数），相当于$max(0,1) + max(0,1) + max(0,1) = 3$
* Q6：范围是$[0,+\infty)$
* Q7：结果是$-log(\frac{1}{3}) = 0.477$

## 2. Optimization
### 2.1 Recap
&nbsp; &nbsp; 现在，我们有了目标了，我们该想想怎么优化我们的$W$使得我们的Loss Function最小了。
### 2.2 闪现
&nbsp; &nbsp; 最朴素的方法无疑是随机选择（Bingo Sort一样）我们其实可以把优化过程看作到达一个山谷，四面八方都是山，我们可以首先在这个山谷里闪现，每到一个地方就看下我们现在的高度（Loss Function）是多少。这确实是一种方法。
### 2.3 梯度下降 Gradient Descent
&nbsp; &nbsp; 这部分在数分里讲过，就不多说了。
#### 2.3.1 数值解
&nbsp; &nbsp; 回顾一下我们对导数的定义
$$\frac{d f(x)}{dx} = \lim_{h \to 0}\frac{f(x + h) - f(x)}{h}$$
类似的，我们有![](https://upload-images.jianshu.io/upload_images/9592233-6cf9e6a7f289d975.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
在我们求出梯度之后，向这个方向走，就可以了。
#### 2.3.2 解析解
&nbsp; &nbsp; 当然数值解很费时间，所以，如果愿意的话，直接求解析解也是可以的，毕竟这整个数学关系式是有的，可以使用链式法则求导。
#### 2.3.3 Learning Rate
&nbsp; &nbsp; 梯度解决了向那里走的问题，走多少呢？学习率，又是一个超参数，走太多，一直在山上下不来了，走太小又耗时间，这确实是个玄学问题。对于这个问题，可以使用学习率衰减（Learning Rate Decay）来“解决”。字面意思，我们越往后训练，走的步长越小。
### 2.4 Mini-Batch Gradient Descent
&nbsp; &nbsp;  在前面我们知道，在损失函数里都有个$\frac{1}{N}$。就是说，我们有时候会把一整个数据集全部算一次，得出一个损失函数，然后根据他来优化，但是，当数据集太大的时候，这一点显然是不适用的，因此，有时候会把数据集分成小块来用，这种做法称为 Mini-Batch Gradient Descent，当每次只使用一个训练样本的时候，称为随机梯度下降（Stochastic Gradient Descent 简称SGD）。